{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lefteryx/RIS-MISO-Deep-Reinforcement-Learning/blob/main/RIS_MISO_Deep_Reinforcement_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f47d119c",
      "metadata": {
        "id": "f47d119c"
      },
      "source": [
        "# RIS-MISO-Deep-Reinforcement-Learning\n",
        "This notebook aims to relate equations in the IEEE-published research paper\n",
        "[Reconfigurable Intelligent Surface Assisted Multiuser MISO Systems Exploiting Deep Reinforcement Learning](https://arxiv.org/abs/2002.10072) to corresponding code in its [PyTorch implementation](https://github.com/baturaysaglam/RIS-MISO-Deep-Reinforcement-Learning)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Equations 1 and 3\n",
        "$$\n",
        "y_{k}=\\mathbf{h}_{k, 2}^{T} \\boldsymbol{\\Phi} \\mathbf{H}_{1} \\mathbf{G x}+w_{k}, \\tag{1}$$\n",
        "$$ and $$ \n",
        "$$\n",
        "y_{k}=\\mathbf{h}_{k, 2}^{T} \\boldsymbol{\\Phi} \\mathbf{H}_{1} \\mathbf{g}_{k} x_{k}+\\sum_{n, n \\neq k}^{K} \\mathbf{h}_{k, 2}^{T} \\boldsymbol{\\Phi} \\mathbf{H}_{1} \\mathbf{g}_{n} x_{n}+w_{k}, \\tag{3}\n",
        "$$\n",
        "\n",
        "Corresponds to code in ```lines 83 to 86``` in ```environment.py``` :  \n",
        "```py\n",
        "x = np.abs(h_2_k.T @ Phi @ self.H_1 @ g_k) ** 2\n",
        "x = x.item()\n",
        "```"
      ],
      "metadata": {
        "id": "YzIsJYArfmFN"
      },
      "id": "YzIsJYArfmFN"
    },
    {
      "cell_type": "markdown",
      "id": "5e0a37cc",
      "metadata": {
        "id": "5e0a37cc"
      },
      "source": [
        "## Equation 4\n",
        "\n",
        "$$\n",
        "\\rho_{k}=\\frac{\\left|\\mathbf{h}_{k, 2}^{T} \\boldsymbol{\\Phi} \\mathbf{H}_{1} \\mathbf{g}_{k}\\right|^{2}}{\\sum_{n, n \\neq k}^{K}\\left|\\mathbf{h}_{k, 2}^{T} \\boldsymbol{\\Phi} \\mathbf{H}_{1} \\mathbf{g}_{n}\\right|^{2}+\\sigma_{n}^{2}}, \\tag{4}\n",
        "$$\n",
        "\n",
        "Corresponds to code in ```lines 89 to 92``` in ```environment.py``` :\n",
        "\n",
        "```py\n",
        "interference = np.sum(np.abs(h_2_k.T @ Phi @ self.H_1 @ G_removed) ** 2)\n",
        "y = interference + (self.K - 1) * self.awgn_var\n",
        "rho_k = x / y\n",
        "```           "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2133f771",
      "metadata": {
        "id": "2133f771"
      },
      "source": [
        "## Equation 5\n",
        "$$\n",
        "C\\left(\\mathbf{G}, \\mathbf{\\Phi}, \\mathbf{h}_{k, 2}, \\mathbf{H}_{1}\\right)=\\sum_{k=1}^{K} R_{k}, \\tag{5}\n",
        "$$\n",
        "\n",
        "\n",
        "$$\\hspace{65mm} where \\space R_k = \\log_2 (1+\\rho_k)$$\n",
        "\n",
        "Corresponds to Code in ```Line 94``` in ```environment.py``` :\n",
        "\n",
        "```py\n",
        "reward += np.log(1 + rho_k) / np.log(2)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Equation 6\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "& \\max _{\\mathbf{G}, \\boldsymbol{\\Phi}} C\\left(\\mathbf{G}, \\mathbf{\\Phi}, \\mathbf{h}_{k, 2}, \\mathbf{H}_{1}\\right) \\\\\n",
        "& \\text { s.t. } \\operatorname{tr}\\left\\{\\mathbf{G G}^{\\mathcal{H}}\\right\\} \\leq P_{t} \\\\\n",
        "& \\left|\\phi_{n}\\right|=1 \\forall n=1,2, \\ldots, N \\text {. }\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Corresponds to code in ```line 95 ``` in file ```environment.py``` :\n",
        "\n",
        "```py\n",
        "opt_reward += np.log(1 + x / ((self.K - 1) * self.awgn_var)) / np.log(2)\n",
        " ```"
      ],
      "metadata": {
        "id": "SZm1jRkP66iR"
      },
      "id": "SZm1jRkP66iR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Equation 7\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "Q_{\\pi}\\left(s^{(t)}, a^{(t)}\\right) & =\\mathcal{E}_{\\pi}\\left[R^{(t)} \\mid s^{(t)}=s, a^{(t)}=a\\right] \\\\\n",
        "R^{(t)} & =\\sum_{\\tau=0}^{\\infty} \\gamma^{\\tau} r^{(t+\\tau+1)},\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "## Equation 8\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "Q_{\\pi}\\left(s^{(t)}, a^{(t)}\\right)= & \\mathcal{E}_{\\pi}\\left[r^{(t+1)} \\mid s^{(t)}=s, a^{(t)}=a\\right] \\\\\n",
        "& +\\gamma \\sum_{s^{\\prime} \\in S} P_{s s^{\\prime}}^{a}\\left(\\sum_{a^{\\prime} \\in A} \\pi\\left(s^{\\prime}, a^{\\prime}\\right) Q^{\\pi}\\left(s^{\\prime}, a^{\\prime}\\right)\\right),\n",
        "\\end{aligned}\n",
        "$$ \n",
        "\n",
        "\n",
        "Both of these equations are pointing out that the Q-function satisfies the Bellman's Equation. This along with equation 10, is implemented as a whole algo in the program about which we studied from the web as Q-Network.\n",
        "\n",
        "Corresponding to code in ```lines 77 to 96``` in file ```DDPG.py``` :\n",
        "\n",
        "```py\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(Critic, self).__init__()\n",
        "        hidden_dim = 1 if (state_dim + action_dim) == 0 else 2 ** ((state_dim + action_dim) - 1).bit_length()\n",
        "\n",
        "        self.l1 = nn.Linear(state_dim, hidden_dim)\n",
        "        self.l2 = nn.Linear(hidden_dim + action_dim, hidden_dim)\n",
        "        self.l3 = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        q = torch.tanh(self.l1(state.float()))\n",
        "\n",
        "        q = self.bn1(q)\n",
        "        q = torch.tanh(self.l2(torch.cat([q, action], 1)))\n",
        "\n",
        "        q = self.l3(q)\n",
        "\n",
        "        return q\n",
        "  ```\n",
        "\n",
        "  Critic Network is basically the Q-Network."
      ],
      "metadata": {
        "id": "ZH6rKwbu67_q"
      },
      "id": "ZH6rKwbu67_q"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Equation 9\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "Q^{*}\\left(s^{(t)}, a^{(t)}\\right)= & r^{(t+1)}\\left(s^{(t)}=s, a^{(t)}, \\pi=\\pi^{*}\\right) \\\\\n",
        "& +\\gamma \\sum_{s^{\\prime} \\in S} P_{s s^{\\prime}}^{a} \\max _{a^{\\prime} \\in A} Q^{*}\\left(s^{\\prime}, a^{\\prime}\\right) .\n",
        "\\end{aligned}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "ODTE9Z2p6-XD"
      },
      "id": "ODTE9Z2p6-XD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Equations 10 and 13\n",
        "\n",
        "\\begin{align}\n",
        "Q^{*}\\left(s^{(t)}, a^{(t)}\\right) \\leftarrow & (1-\\alpha) Q^{*}\\left(s^{(t)}, a^{(t)}\\right)+\\alpha\\left(r^{(t+1)}\\right. \n",
        " \\left.+\\gamma \\max_{a^{\\prime}} Q_{\\pi}\\left(s^{(t+1)}, a^{\\prime}\\right)\\right) \\tag{10}\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "$$$$\n",
        "\n",
        "$$\n",
        "y=r^{(t+1)}+\\gamma \\max _{a^{\\prime}} Q\\left(\\theta^{(\\text {target })} \\mid s^{(t+1)}, a^{\\prime}\\right), \\tag{13}\n",
        "$$\n",
        "\n",
        "Corresponds to code in ```lines 134 and 135``` in file ```DDPG.py``` :\n",
        "\n",
        "```py\n",
        "target_Q = self.critic_target(next_state, self.actor_target(next_state))\n",
        "target_Q = reward + (not_done * self.discount * target_Q).detach()\n",
        "```"
      ],
      "metadata": {
        "id": "AO731rLT2LP2"
      },
      "id": "AO731rLT2LP2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Equation 11\n",
        "\n",
        "$$\n",
        "Q(s(t), a(t)) \\triangleq Q(\\theta \\mid s(t), a(t))\n",
        "$$"
      ],
      "metadata": {
        "id": "01OskD0y6s-J"
      },
      "id": "01OskD0y6s-J"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Equation 12\n",
        "\n",
        "$$\n",
        "\\theta^{(t+1)}=\\theta^{(t)}-\\mu \\Delta_{\\theta} \\ell(\\theta)\n",
        "$$\n",
        "\n",
        "Corresponds to code in ```lines 108 and 113 ``` in file ```DDPG.py``` :\n",
        "\n",
        "```py\n",
        "#108\n",
        "self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=actor_lr, weight_decay=actor_decay)\n",
        "#113\n",
        "self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=critic_lr, weight_decay=critic_decay)\n",
        "```\n",
        "\n",
        "Because Adam is the stochastic optimization\n",
        "algorithm being used."
      ],
      "metadata": {
        "id": "cuVxXxap6vn9"
      },
      "id": "cuVxXxap6vn9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Equations 14-16\n",
        "\n",
        "$$\n",
        "\\ell(\\theta)=\\left(y-Q\\left(\\theta^{(\\text {train })} \\mid s^{(t)}, a^{(t)}\\right)\\right)^{2}, \\tag{14}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\theta_{c}^{(t+1)}= \\theta_{c}^{(t)}-\\mu_{c} \\Delta_{\\theta_{c}^{(\\text {train })}} \\ell\\left(\\theta_{c}^{(\\text {train})}\\right), \\tag{15}\n",
        "$$\n",
        "\n",
        "$$$$\n",
        "\n",
        "$$\n",
        "\\ell\\left(\\theta_{c}^{(\\text {train})}\\right)= \\left(r^{(t)}+\\gamma q\\left(\\theta_{c}^{(\\text {target})} \\mid s^{(t+1)}, a^{\\prime}\\right)-q\\left(\\theta_{c}^{(\\text {train})} \\mid s^{(t)}, a^{(t)}\\right)\\right)^{2}, \\tag{16}\n",
        "$$\n",
        "\n",
        "Corresponds to code in ```lines 105-113, 140, 141``` in file ```DDPG.py``` :\n",
        "    # Initialize actor networks and optimizer\n",
        "    self.actor = Actor(state_dim, action_dim, M, N, K, powert_t_W, max_action=max_action, device=device).to(self.device)\n",
        "    self.actor_target = copy.deepcopy(self.actor)\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=actor_lr, weight_decay=actor_decay)\n",
        "\n",
        "    # Initialize critic networks and optimizer\n",
        "    self.critic = Critic(state_dim, action_dim).to(self.device)\n",
        "    self.critic_target = copy.deepcopy(self.critic)\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=critic_lr, weight_decay=critic_decay)\n",
        "\n",
        "    ...\n",
        "\n",
        "    # Compute the critic loss\n",
        "    critic_loss = F.mse_loss(current_Q, target_Q)"
      ],
      "metadata": {
        "id": "p5o8U2Jb6xoW"
      },
      "id": "p5o8U2Jb6xoW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Equation 17\n",
        "\n",
        "$$\n",
        "\\theta_{a}^{(t)}-\\mu_{a} \\Delta_{a} q\\left(\\theta_{c}^{(\\text {target })} \\mid s^{(t)}, a\\right) \\Delta_{\\theta_{a}^{(\\text {train })}} \\pi\\left(\\theta_{a}^{(\\text {train })} \\mid s^{(t)}\\right), \\tag{17}\n",
        "$$"
      ],
      "metadata": {
        "id": "7ZtKM3AS61if"
      },
      "id": "7ZtKM3AS61if"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Equation 18\n",
        "\n",
        "$$ \n",
        "\\theta_{c}^{\\text ({target})} \\leftarrow \\tau_{c} \\theta_{c}^{(\\text {train})}+\\left(1-\\tau_{c}\\right) \\theta_{c}^{\\text {(target})},\n",
        "$$\n",
        "\n",
        "$$ \n",
        "\\theta_{a}^{(\\text {target })} \\leftarrow \\tau_{a} \\theta_{a}^{(\\text {train })}+\\left(1-\\tau_{a}\\right) \\theta_{a}^{(\\text {target})}, \\tag{18}\n",
        "$$\n",
        "\n",
        "Corresponds to code in ```lines 156-161``` in file ```DDPG.py``` :\n",
        "\n",
        "        # Soft update the target networks\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)"
      ],
      "metadata": {
        "id": "g5mF3IZQ627K"
      },
      "id": "g5mF3IZQ627K"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}